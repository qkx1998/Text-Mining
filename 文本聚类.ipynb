{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77821ee4",
   "metadata": {},
   "source": [
    "### 文本聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835f2d8",
   "metadata": {},
   "source": [
    "### **tfidf+kmeans, Single-pass clustering, dbscan, 文本层次聚类**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b768a",
   "metadata": {},
   "source": [
    "在商业语境下也叫“典型意见挖掘”  \n",
    "从大量无标签的文本数据中发现其潜在的结构，根据文本数据的结构特征“归纳”出若干个主题或者意见。  \n",
    "   \n",
    "一般情况下文本聚类包含以下步骤：  \n",
    "step1: 文本预处理（切词、去除停用词、数字归一化等）  \n",
    "step2: 构建向量空间模型(Vector Space Model，VSS)  \n",
    "step3: 使用LSA / SVD对文档向量降维（可选，当维度过高时可以选用）  \n",
    "step4: 使用肘部方法（Elbow Method） 去发现最优的聚类数量  \n",
    "step5: 按照得出的最优聚类数，使用K-means或Minibatch K-means进行文本聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383edfb8",
   "metadata": {},
   "source": [
    "#### TFIDF + KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81128687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本特征提取 tfidf\n",
    "vectorizer = NumberNormalizingVectorizer(max_df=0.5, max_features=n_features,\n",
    "                                         min_df=2, stop_words=stwlist,ngram_range=(1, 2),\n",
    "                                         use_idf=opts.use_idf)\n",
    "\n",
    "X = vectorizer.fit_transform(data['正文切词'])\n",
    "\n",
    "#Vectorizer的结果被归一化，这使得KMeans表现为球形k均值（Spherical K-means）以获得更好的结果。 \n",
    "#由于LSA / SVD结果并未标准化，我们必须重做标准化。\n",
    "svd = TruncatedSVD(opts.n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"SVD解释方差的step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008efbc5",
   "metadata": {},
   "source": [
    "使用肘部方法（Elbow Method） 发现最佳聚类数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e0ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters= 40\n",
    "\n",
    "wcss = []\n",
    "for i in range(1,n_clusters):\n",
    "if opts.minibatch:\n",
    "        km = MiniBatchKMeans(n_clusters=i, init='k-means++', n_init=2,n_jobs=-1,\n",
    "                         init_size=1000, batch_size=1500, verbose=opts.verbose)\n",
    "else:\n",
    "        km = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=2,n_jobs=-1,\n",
    "                verbose=opts.verbose)\n",
    "    km.fit(X)\n",
    "    wcss.append(km.inertia_)\n",
    "plt.plot(range(1,n_clusters),wcss)\n",
    "plt.title('肘 部 方 法')\n",
    "plt.xlabel('聚类的数量')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9642495",
   "metadata": {},
   "source": [
    "很多情况下，并不会有明显的拐点出现  \n",
    "笔者比较提倡的做法还是从实际问题出发，人工指定比较合理的K值，通过多次随机初始化聚类中心选取比较满意的结果。  \n",
    "在这里，笔者选择30作为聚类数，先试试效果，如果效果不好，再试试18、25等聚类数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ba00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=300, n_init=5,n_jobs=-1,\n",
    "            verbose=opts.verbose)\n",
    "\n",
    "km.fit(X)\n",
    "\n",
    "print(\"Homogeneity值: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness值: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure值: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index值: %.3f\"% metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient值: %0.3f\"% metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca4b5c",
   "metadata": {},
   "source": [
    "#### Single-pass clustering 单遍聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091d863",
   "metadata": {},
   "source": [
    "它是一种简洁且高效的文本聚类算法。在文本主题聚类中，Single-pass聚类算法比K-means来的更为有效。\n",
    "Single-pass聚类算法不需要指定类目数量，可以通过设定相似度阈值来限定聚类数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import jieba\n",
    "import json\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from smart_open import  smart_open\n",
    "import pandas as pd\n",
    "from pyltp import SentenceSplitter  #按标点切分语句\n",
    "from  textrank4zh import TextRank4Keyword,TextRank4Sentence #关键词和关键句提取\n",
    "from tkinter import _flatten   #用于将嵌套列表压成一层\n",
    "\n",
    "class Single_Pass_Cluster(object):\n",
    "    def __init__(self, \n",
    "                 filename, \n",
    "                 stop_words_file= '停用词汇总.txt',\n",
    "                 theta = 0.5):\n",
    "\n",
    "        self.filename = filename\n",
    "        self.stop_words_file = stop_words_file\n",
    "        self.theta = theta \n",
    "\n",
    "    def loadData(self,filename):\n",
    "        '''以列表的形式读取文档'''\n",
    "        Data = []\n",
    "        i = 0\n",
    "        with smart_open(self.filename,encoding='utf-8') as f:    \n",
    "            #鉴于有些文档较长，包含多个语义中心，因此按语句结束标点进行切割获取表意单一的句子产生的聚类效果会更好    \n",
    "            texts = [list(SentenceSplitter.split(i.strip().strip('\\ufeff'))) for i in f.readlines()]\n",
    "            print('未切割前的语句总数有{}条...'.format(len(texts)))\n",
    "            print (\"............................................................................................\")  \n",
    "            texts = [i.strip() for i in list(_flatten(texts)) if len(i)>5]\n",
    "            print('切割后的语句总数有{}条...'.format(len(texts)))\n",
    "            for line in texts:\n",
    "                i  += 1\n",
    "                Data.append(line )\n",
    "        return Data\n",
    "\n",
    "    def word_segment(self,texts):\n",
    "        '''对语句进行分词，并去掉常见无意义的高频词（停用词）'''\n",
    "        stopwords = [line.strip() for line in open( self.stop_words_file,encoding='utf-8').readlines()]\n",
    "        word_segmentation = []\n",
    "        words = jieba.cut(texts)\n",
    "        for word in words:\n",
    "            if word == ' ':\n",
    "                continue\n",
    "            if word not in stopwords:\n",
    "                word_segmentation.append(word)\n",
    "        return word_segmentation\n",
    "\n",
    "    def get_Tfidf_vector_representation(self,word_segmentation,pivot= 10, slope = 0.1):\n",
    "        '''采用VSM(vector space model)得到文档的空间向量表示，也可以doc2vec等算法直接获取句向量'''\n",
    "        dictionary = corpora.Dictionary(word_segmentation)  #获取分词后词汇和词汇id的映射关系，形成字典\n",
    "        corpus = [dictionary.doc2bow(text) for text in word_segmentation]   #得到语句的向量表示\n",
    "        tfidf = models.TfidfModel(corpus,pivot=pivot, slope =slope)      #进一步获取语句的TF-IDF向量表示\n",
    "        corpus_tfidf = tfidf[corpus]\n",
    "        return corpus_tfidf\n",
    "\n",
    "    def getMaxSimilarity(self,dictTopic, vector):\n",
    "        '''计算新进入文档和已有文档的文本相似度，这里采用的是cosine余弦相似度，还可以试试kullback_leibler, jaccard, hellinger等'''\n",
    "        maxValue = 0\n",
    "        maxIndex = -1\n",
    "        for k,cluster in dictTopic.items():\n",
    "            oneSimilarity = np.mean([matutils.cossim(vector, v) for v in cluster])\n",
    "            if oneSimilarity > maxValue:\n",
    "                maxValue = oneSimilarity\n",
    "                maxIndex = k\n",
    "        return maxIndex, maxValue\n",
    "\n",
    "    def single_pass(self,corpus,texts,theta):\n",
    "        dictTopic = {}\n",
    "        clusterTopic = {}\n",
    "        numTopic = 0 \n",
    "        cnt = 0\n",
    "        for vector,text in zip(corpus,texts): \n",
    "            if numTopic == 0:\n",
    "                dictTopic[numTopic] = []\n",
    "                dictTopic[numTopic].append(vector)\n",
    "                clusterTopic[numTopic] = []\n",
    "                clusterTopic[numTopic].append(text)\n",
    "                numTopic += 1\n",
    "            else:\n",
    "                maxIndex, maxValue = self.getMaxSimilarity(dictTopic, vector)\n",
    "                # 以第一篇文档为种子，建立一个主题，将给定语句分配到现有的、最相似的主题中\n",
    "                if maxValue > theta:\n",
    "                    dictTopic[maxIndex].append(vector)\n",
    "                    clusterTopic[maxIndex].append(text)\n",
    "\n",
    "                # 或者创建一个新的主题\n",
    "                else:\n",
    "                    dictTopic[numTopic] = []\n",
    "                    dictTopic[numTopic].append(vector)\n",
    "                    clusterTopic[numTopic] = []\n",
    "                    clusterTopic[numTopic].append(text)\n",
    "                    numTopic += 1\n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                print (\"processing {}...\".format(cnt))\n",
    "        return dictTopic, clusterTopic  \n",
    "\n",
    "    def fit_transform(self,theta=0.5):\n",
    "        '''综合上述的函数，得出最终的聚类结果：包括聚类的标号、每个聚类的数量、关键主题词和关键语句'''\n",
    "        datMat = self.loadData(self.filename)  \n",
    "        word_segmentation = []\n",
    "        for i in range(len(datMat)):\n",
    "            word_segmentation.append(self.word_segment(datMat[i]))          \n",
    "        print (\"............................................................................................\")  \n",
    "        print('文本已经分词完毕 !')\n",
    "\n",
    "        #得到文本数据的空间向量表示\n",
    "        corpus_tfidf = self.get_Tfidf_vector_representation(word_segmentation)\n",
    "        dictTopic, clusterTopic = self.single_pass(corpus_tfidf, datMat, theta)\n",
    "        print (\"............................................................................................\")  \n",
    "        print( \"得到的主题数量有: {} 个 ...\".format(len(dictTopic)))\n",
    "        print (\"............................................................................................\\n\")  \n",
    "        #按聚类语句数量对聚类结果进行降序排列，找到重要的聚类群\n",
    "        clusterTopic_list = sorted(clusterTopic.items(),key=lambda x: len(x[1]),reverse=True)\n",
    "        for k in clusterTopic_list:\n",
    "            cluster_title = '\\n'.join(k[1]) \n",
    "            # 得到每个聚类中的主题关键词\n",
    "            word = TextRank4Keyword()\n",
    "            word.analyze(''.join(self.word_segment(''.join(cluster_title))),window = 5,lower = True)\n",
    "            w_list = word.get_keywords(num = 10,word_min_len = 2)\n",
    "           # 得到每个聚类中的关键主题句TOP3\n",
    "            sentence = TextRank4Sentence()\n",
    "            sentence.analyze(' '.join(k[1]) ,lower = True)\n",
    "            s_list = sentence.get_key_sentences(num = 3,sentence_min_len = 3)\n",
    "            print (\"【主题索引】:{} \\n【主题语量】：{} \\n【主题关键词】：{} \\n【主题中心句】 ：\\n{}\".format(k[0],len(k[1]),','.join([i.word for i in w_list]),'\\n'.join([i.sentence for i in s_list])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2038386",
   "metadata": {},
   "source": [
    "#### dbscan聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4304bb",
   "metadata": {},
   "source": [
    "由于 DBSCAN不能很好反映高维数据，所以对抽取的特征进行降维是很有必要的.这里采用的是LSA降维，暂时设定15维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d065950",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                             max_features=40000,\n",
    "                             min_df=5, \n",
    "                             stop_words=stwlist,ngram_range=(1, 2),\n",
    "                             use_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(data['正文切词'])\n",
    "\n",
    "svd = TruncatedSVD(15)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "db = DBSCAN(eps=0.2, min_samples=4).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73668a7",
   "metadata": {},
   "source": [
    "聚类数及噪点计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('聚类数：',n_clusters_)\n",
    "print('噪点数：',n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256259f",
   "metadata": {},
   "source": [
    "#### 文本层次聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(sentences)\n",
    "\n",
    "# 计算成对的余弦相似度，也就是计算出一个相似度矩阵\n",
    "sims = cosine_similarity(X)\n",
    "# 对相似度矩阵中每个cell的值保留5位小数，确保精度不损失太多：\n",
    "similarity = np.round(sims, decimals = 5)\n",
    "\n",
    "# \n",
    "cluster = AgglomerativeClustering(\n",
    "                                  n_clusters = None,\n",
    "                                  distance_threshold= 0,\n",
    "                                  affinity = \"cosine\",\n",
    "                                  linkage = \"average\"\n",
    "                                                    )  \n",
    "cluster.fit(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99351789",
   "metadata": {},
   "source": [
    "绘制树状图的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "      # 建立邻接矩阵(linkage matrix)，然后绘制树状图（dendrogram）\n",
    "\n",
    "   # 创建每个节点下的样本数\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # 绘制对应的树状图\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34105c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制树状图的前14层\n",
    "plot_dendrogram(cluster, \n",
    "                truncate_mode='level',\n",
    "                p=14, \n",
    "                leaf_font_size=15,\n",
    "               )\n",
    "\n",
    "plt.xlabel(\"节点中的点的数量（如果没有括号，则为语句对应的索引）\",size = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9218b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
