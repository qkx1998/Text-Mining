{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49eb7bfa",
   "metadata": {},
   "source": [
    "### ptm参考：https://github.com/dongwookim-ml/python-topic-model/tree/master/ptm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302633c",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from ptm import GibbsLDA\n",
    "from ptm import vbLDA\n",
    "from ptm.nltk_corpus import get_reuters_ids_cnt\n",
    "from ptm.utils import convert_cnt_to_list, get_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6791431",
   "metadata": {},
   "source": [
    "Loading Reuter corpus from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b48bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_doc = 1000\n",
    "voca, doc_ids, doc_cnt = get_reuters_ids_cnt(num_doc=n_doc, max_voca=10000)\n",
    "docs = convert_cnt_to_list(doc_ids, doc_cnt)\n",
    "n_voca = len(voca)\n",
    "print('Vocabulary size:%d' % n_voca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efd837",
   "metadata": {},
   "source": [
    "Inferencen through the Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter=100\n",
    "n_topic=10\n",
    "\n",
    "logger = logging.getLogger('GibbsLDA')\n",
    "logger.propagate = False\n",
    "\n",
    "model = GibbsLDA(n_doc, len(voca), n_topic)\n",
    "model.fit(docs, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251f74b",
   "metadata": {},
   "source": [
    "Inferencen through the Variational Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('vbLDA')\n",
    "logger.propagate = False\n",
    "\n",
    "vbmodel = vbLDA(n_doc, n_voca, n_topic)\n",
    "vbmodel.fit(doc_ids, doc_cnt, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811ed79",
   "metadata": {},
   "source": [
    "Print top 10 probability words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ti in range(n_topic):\n",
    "    top_words = get_top_words(vbmodel._lambda, voca, ti, n_words=10)\n",
    "    print('Topic', ti ,': ', ','.join(top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70883f0",
   "metadata": {},
   "source": [
    "### HMM-LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from ptm.nltk_corpus import get_reuters_token_list_by_sentence\n",
    "from ptm import HMM_LDA\n",
    "from ptm.utils import get_top_words\n",
    "\n",
    "logger = logging.getLogger('HMM_LDA')\n",
    "logger.propagate=False\n",
    "\n",
    "n_docs = 1000\n",
    "voca, corpus = get_reuters_token_list_by_sentence(num_doc=n_docs)\n",
    "print('Vocabulary size', len(voca))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af048249",
   "metadata": {},
   "source": [
    "Training HMM LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696de0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = len(corpus)\n",
    "n_voca = len(voca)\n",
    "n_topic = 50\n",
    "n_class = 20\n",
    "max_iter = 100\n",
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "gamma = 0.1\n",
    "eta = 0.1\n",
    "model = HMM_LDA(n_docs, n_voca, n_topic, n_class, alpha=alpha, beta=beta, gamma=gamma, eta=eta, verbose=False)\n",
    "model.fit(corpus, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b59173",
   "metadata": {},
   "source": [
    "Print Top 10 words for each class and topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d076c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ti in range(n_topic):\n",
    "    top_words = get_top_words(model.TW, voca, ti, n_words=10)\n",
    "    print('Topic', ti ,': ', ','.join(top_words))\n",
    "    \n",
    "for ci in range(1, n_class):\n",
    "    top_words = get_top_words(model.CW, voca, ci, n_words=10)\n",
    "    print('Class', ci ,': ', ','.join(top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328c3cc",
   "metadata": {},
   "source": [
    "### AuthorTopicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptm import AuthorTopicModel\n",
    "\n",
    "logger = logging.getLogger('AuthorTopicModel')\n",
    "logger.propagate=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e84e76",
   "metadata": {},
   "source": [
    "Load CORA dataset: https://people.cs.umass.edu/~mccallum/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed461e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = pickle.load(open('../data/cora/doc_ids.pkl', 'rb'))\n",
    "doc_cnt = pickle.load(open('../data/cora/doc_cnt.pkl', 'rb'))\n",
    "doc_author = pickle.load(open('../data/cora/doc_authorid.pkl', 'rb'))\n",
    "author_name = pickle.load(open('../data/cora/authorid_authorname.pkl', 'rb'))\n",
    "voca = pickle.load(open('../data/cora/voca.pkl', 'rb'))\n",
    "\n",
    "corpus = convert_cnt_to_list(doc_ids, doc_cnt)\n",
    "n_doc = len(corpus)\n",
    "n_topic = 10\n",
    "n_author = len(author_name)\n",
    "n_voca = len(voca)\n",
    "max_iter = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1925f5",
   "metadata": {},
   "source": [
    "Fit author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AuthorTopicModel(n_doc, n_voca, n_topic, n_author)\n",
    "model.fit(corpus, doc_author, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adc7dc",
   "metadata": {},
   "source": [
    "Print top 10 words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(n_topic):\n",
    "    top_words = get_top_words(model.TW, voca, k, 10)\n",
    "    print('topic ', k , ','.join(top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf05129",
   "metadata": {},
   "source": [
    "### Relational Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptm import RelationalTopicModel\n",
    "\n",
    "model = RelationalTopicModel(n_topic, n_doc, n_voca, verbose=True)\n",
    "model.fit(doc_ids, doc_cnt, doc_links, max_iter=max_iter)\n",
    "\n",
    "for k in range(n_topic):\n",
    "    top_words = get_top_words(model.beta, voca, k, 10)\n",
    "    print('Topic', k, ':', ','.join(top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d2df3",
   "metadata": {},
   "source": [
    "### Supervised Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9855100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptm import GibbsSupervisedLDA\n",
    "\n",
    "n_topic = 50\n",
    "r_var = 0.01\n",
    "\n",
    "model = GibbsSupervisedLDA(n_doc, n_voca, n_topic, sigma=r_var)\n",
    "model.fit(corpus, ratings)\n",
    "\n",
    "for ti in model.eta.argsort():\n",
    "    top_words = get_top_words(model.TW, voca, ti, n_words=10)\n",
    "    print('Eta', model.eta[ti] ,'Topic', ti ,':\\t', ','.join(top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa5ae0",
   "metadata": {},
   "source": [
    "#### 以下参考 https://mp.weixin.qq.com/s/PMWAyytQjSGspAxXKsDx6w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc42ad",
   "metadata": {},
   "source": [
    "### Biterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53773062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BTM(object):\n",
    "    def __init__(self, data_path, alpha, beta, num_iter, num_topic, output_dir):\n",
    "        self.data_path = data_path\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.num_iter = num_iter\n",
    "        self.num_topic = num_topic\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        self.word2Id = {}\n",
    "        self.Id2Word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        self.wordId_corpus = []\n",
    "        \n",
    "        self.biterms_in_doc = [] #list of dictionaries long->int\n",
    "        self.num_doc_biterm = defaultdict(int)\n",
    "        self.biterms = [] #List of numbers\n",
    "\n",
    "        self.topic_biterm = []\n",
    "        self.topic_word_num = [] #list of lists\n",
    "        self.num_topic_biterm = []\n",
    "        \n",
    "        self.biterm_sum = {} #Map from long to double\n",
    "        \n",
    "    def get_file_reader(self, path = None):\n",
    "        if path is None:\n",
    "            path = self.data_path\n",
    "        f = open(path, 'r')\n",
    "        return f\n",
    "    \n",
    "    def get_file_writer(self,path, append = False):\n",
    "        if append:\n",
    "            read_mode = 'a'\n",
    "        else:\n",
    "            read_mode = 'w'\n",
    "        g = open(os.path.join(self.output_dir, path), read_mode)\n",
    "        return g\n",
    "    \n",
    "    def print_params(self):\n",
    "        params = ['alpha','beta','num_iter','num_topic','topic_word_num','num_topic_biterm','topic_biterm']\n",
    "        for param in params:\n",
    "            print(param,':',getattr(self, param))\n",
    "            print('-'*40)\n",
    "    \n",
    "    def load_data(self):\n",
    "        f = self.get_file_reader()\n",
    "        for line in f.readlines():\n",
    "            words = line.split()\n",
    "            curr_doc = []\n",
    "            for word in words:\n",
    "                if word not in self.word2Id:\n",
    "                    index = len(self.word2Id)\n",
    "                    self.word2Id[word] = index\n",
    "                    self.Id2Word[index] = word\n",
    "                curr_doc.append(self.word2Id[word])\n",
    "            self.wordId_corpus.append(curr_doc)\n",
    "        f.close()\n",
    "        \n",
    "        self.num_doc_biterm = [0]*len(self.wordId_corpus)\n",
    "    \n",
    "    def init_model(self):\n",
    "        for doc_number, doc in enumerate(self.wordId_corpus):\n",
    "            oneCop = defaultdict(int)\n",
    "            for word1 in doc:\n",
    "                for word2 in doc:\n",
    "                    if(word1<word2):\n",
    "                        item_num = word1*1000000+word2 #encoding the biterms\n",
    "                        oneCop[item_num] +=1\n",
    "                        self.biterms.append(item_num)\n",
    "                        self.num_doc_biterm[doc_number] +=1\n",
    "            self.biterms_in_doc.append(oneCop)\n",
    "            \n",
    "        self.vocab_size = len(self.word2Id)\n",
    "        \n",
    "        self.topic_biterm = [0]*len(self.biterms)\n",
    "        self.topic_word_num = {j: {i:0 for i in range(self.num_topic)} for j in range(self.vocab_size)}\n",
    "        print(len(self.topic_word_num), len(self.topic_word_num[0]))\n",
    "        self.num_topic_biterm = [1]*self.num_topic\n",
    "        \n",
    "        for biterm_index, biterm in enumerate(self.biterms):\n",
    "            topic_id = random.randint(0, self.num_topic-1)\n",
    "            #if biterm_index  5:\n",
    "                #print(biterm, biterm%1000000, biterm//1000000)\n",
    "                #print(self.topic_word_num)\n",
    "            self.topic_word_num[biterm%1000000][topic_id] +=1\n",
    "            self.topic_word_num[biterm//1000000][topic_id] +=1\n",
    "            self.topic_biterm[biterm_index] = topic_id\n",
    "            \n",
    "    def save_topic_words(self, topic_word_num = 10):\n",
    "        writer = self.get_file_writer(path = 'model-final-topic-words.txt')\n",
    "        for topic_id in range(self.num_topic):\n",
    "            topic_line = {}\n",
    "            for word_id, word in enumerate(self.word2Id):\n",
    "                topic_line[word_id] = self.topic_word_num[word_id][topic_id]/ self.num_topic_biterm[topic_id] / 2\n",
    "            sorted_topic_line = sorted(topic_line.items(), key = operator.itemgetter(1) )\n",
    "            writer.write(\"Topic:\"+str(topic_id) + '\\n')\n",
    "            for topic_word,score in sorted_topic_line[:topic_word_num]:\n",
    "                writer.write(\"\\t\"+str(self.Id2Word[topic_word])+\"\\t\"+str(score) + '\\n')\n",
    "        writer.close()\n",
    "    \n",
    "    def save_wordIds(self):\n",
    "        writer = self.get_file_writer(path = 'model-final-wordIds.txt')\n",
    "        for key,value in self.word2Id.items():\n",
    "            writer.write(str(key) + ' ' + str(value) + '\\n')\n",
    "        writer.close()\n",
    "        \n",
    "    def get_sum(self, biterm):\n",
    "        if biterm not in self.biterm_sum:\n",
    "            word1 = biterm//1000000\n",
    "            word2 = biterm%1000000\n",
    "            sum = 0\n",
    "            for topic_id in range(self.num_topic):\n",
    "                calculation = (self.num_topic_biterm[topic_id] + self.alpha) * (self.topic_word_num[word1][topic_id] + self.beta) * (self.topic_word_num[word2][topic_id] + self.beta) / ((2 * self.num_topic_biterm[topic_id] ) + (self.vocab_size * self.beta))**2\n",
    "                sum += calculation\n",
    "            self.biterm_sum[biterm] = sum\n",
    "        return self.biterm_sum[biterm]\n",
    "        \n",
    "    def save_theta(self):\n",
    "        writer = self.get_file_writer(path = 'model-final-theta.txt')\n",
    "\n",
    "        for doc_index, line in enumerate(self.biterms_in_doc):\n",
    "            for topic_id in range(self.num_topic):\n",
    "                one_sum = 0\n",
    "                for key in line:\n",
    "                    word1 = key//1000000\n",
    "                    word2 = key%1000000\n",
    "                    one_sum += ((line[key]/self.num_doc_biterm[doc_index]) * ((self.num_topic_biterm[topic_id] + self.alpha) * (self.topic_word_num[word1][topic_id] + self.beta) * (self.topic_word_num[word2][topic_id] + self.beta) / ((2 * self.num_topic_biterm[topic_id] ) + (self.vocab_size * self.beta))**2)/(self.get_sum(key)))\n",
    "                writer.write(str(one_sum) + \" \")\n",
    "            writer.write('\\n')\n",
    "        writer.close()\n",
    "        \n",
    "    def save_phi(self):\n",
    "        writer = self.get_file_writer(path = 'model-final-phi.txt')\n",
    "        for topic_id in range(self.num_topic):\n",
    "            for word_id in self.Id2Word:\n",
    "                calculation = (self.topic_word_num[word_id][topic_id] + self.beta) / ((self.num_topic_biterm[topic_id] * 2) + (self.vocab_size * self.beta))\n",
    "                writer.write(str(calculation) + ' ')\n",
    "            writer.write('\\n')\n",
    "        writer.close()\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        for it in range(self.num_iter):\n",
    "            start_time = time.time()\n",
    "            for biterm_index, old_topic_id in enumerate(self.topic_biterm):\n",
    "                word1 = self.biterms[biterm_index]//1000000\n",
    "                word2 = self.biterms[biterm_index]%1000000\n",
    "                self.topic_word_num[word1][old_topic_id] -=1\n",
    "                self.topic_word_num[word2][old_topic_id] -=1\n",
    "                self.num_topic_biterm[old_topic_id] -=1\n",
    "                \n",
    "                new_topic_id = -1\n",
    "                \n",
    "                p = [0]*self.num_topic\n",
    "                for k in range(self.num_topic):\n",
    "                    p[k] = (self.num_topic_biterm[k] + self.alpha) * (self.topic_word_num[word1][k] + self.beta) * (self.topic_word_num[word2][k] + self.beta) / ((2 * self.num_topic_biterm[k] ) + (self.vocab_size * self.beta))**2\n",
    "                    \n",
    "                for k in range(1,self.num_topic):\n",
    "                    p[k] += p[k-1]\n",
    "                \n",
    "                u = random.random() * p[-1]\n",
    "                for k in range(self.num_topic):\n",
    "                    if u < p[k]:\n",
    "                        new_topic_id = k\n",
    "                        break\n",
    "                \n",
    "                self.topic_word_num[word1][new_topic_id] +=1\n",
    "                self.topic_word_num[word2][new_topic_id] +=1\n",
    "                self.num_topic_biterm[new_topic_id] += 1\n",
    "                \n",
    "                self.topic_biterm[biterm_index] = new_topic_id\n",
    "                \n",
    "            print('Finished iteration:', it, 'Time taken:' + str(time.time()-start_time))\n",
    "    \n",
    "    def save_result(self):\n",
    "        self.save_topic_words(20)\n",
    "        self.save_theta()\n",
    "        self.save_wordIds()\n",
    "        self.save_phi()\n",
    "        \n",
    "    def run(self):\n",
    "        self.load_data()\n",
    "        self.init_model()\n",
    "        self.build_model()\n",
    "        self.save_result() \n",
    "        \n",
    "btm = BTM(data_path='../Data/sample-data.txt',alpha=2,beta=0.001, num_iter=100, num_topic=50, output_dir='.')\n",
    "btm.run()\n",
    "btm.save_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482d52f",
   "metadata": {},
   "source": [
    "version2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91045ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from numpy import fromiter, zeros\n",
    "from numpy.random import choice, randint\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "class BitermModel:\n",
    "    \"\"\"\n",
    "    Biterm model for small documents.\n",
    "    Parameters are:\n",
    "        text: a list of lists where each element is the tokens of a document\n",
    "            gensim.utils.simple_tokenize is a good choice\n",
    "            Note: best results will come from removing stopwords as well\n",
    "        ntopics: the number of topics to infer\n",
    "        alpha: dirichlet prior hyperparameter for topic distribution\n",
    "        beta: dirichlet prior hyperparameter for word distribution\n",
    "        niter: number of gibbs sampling steps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text, ntopics=5, alpha=0.001, beta=0.001, niter=1):\n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.biterms, self.nwords, self.vocab = self._fit_corpus(text)\n",
    "        self.topics, self.topic_words = self._gibbs_sample(niter)\n",
    "        self.text = text\n",
    "        \n",
    "    def _flatten(self, l):\n",
    "        return [item for sublist in l for item in sublist]\n",
    "    \n",
    "    def _normalize(self, v):\n",
    "        c = v.sum()\n",
    "        return v / c\n",
    "        \n",
    "    def _ngrams(self, sequence, n):\n",
    "        return zip(*[sequence[i:] for i in range(n)])\n",
    "\n",
    "    def _skipgrams(self, sequence, n, k):\n",
    "        grams = []\n",
    "        for ngram in self._ngrams(sequence + [None]*k, n + k):\n",
    "            head = ngram[:1]\n",
    "            tail = ngram[1:]\n",
    "            for skip_tail in combinations(tail, n - 1):\n",
    "                if skip_tail[-1] is None:\n",
    "                    continue\n",
    "                grams.append(head + skip_tail)\n",
    "        return grams\n",
    "\n",
    "    def _fit_corpus(self, text):\n",
    "        skip2grams = []\n",
    "        biterms = []\n",
    "        for doc in text:\n",
    "            skip2doc = self._skipgrams(doc, 2, 1)\n",
    "            skip2grams.extend(skip2doc)\n",
    "            for skip in skip2doc:\n",
    "                i, j = skip\n",
    "                if i == j:\n",
    "                    continue\n",
    "                b = (i, j) if i < j else (j, i)\n",
    "                biterms.append(b)\n",
    "            \n",
    "        nwords = sum(len(doc) for doc in text)\n",
    "        vocab = frozenset(self._flatten(text))\n",
    "        \n",
    "        return biterms, nwords, vocab\n",
    "\n",
    "    def _gibbs_sample(self, niter):\n",
    "        a = self.alpha\n",
    "        b = self.beta\n",
    "        K = self.ntopics\n",
    "        M = self.nwords\n",
    "        V = self.vocab\n",
    "        \n",
    "        def z_posterior(n_z, n_wiz, n_wjz):\n",
    "            p = (n_z + a)*(n_wiz + b)*(n_wjz + b)/((2*n_z + M*b + 1)*(2*n_z + M*b))\n",
    "            return p\n",
    "        \n",
    "        def theta_z(z):\n",
    "            n_b = sum(n_z.values())\n",
    "            return (n_z[z] + a) / (n_b + K*a)\n",
    "        \n",
    "        def phi_kw(z, w):\n",
    "            return (n_wz[w][z] + b) / (2*n_z[z] + M*b)\n",
    "    \n",
    "        n_z = defaultdict(int)\n",
    "        n_wz = {word: defaultdict(int) for word in V}\n",
    "        current_assignments = []\n",
    "        for bi in self.biterms:\n",
    "            wi, wj = bi\n",
    "            z_init = randint(K)\n",
    "            \n",
    "            current_assignments.append((bi, z_init))\n",
    "            n_z[z_init] += 1\n",
    "            n_wz[wi][z_init] += 1\n",
    "            n_wz[wj][z_init] += 1\n",
    "        \n",
    "        for _ in range(niter):\n",
    "            for i, (bi, z) in enumerate(current_assignments):\n",
    "                wi, wj = bi\n",
    "    \n",
    "                n_z[z] -= 1\n",
    "                n_wz[wi][z] -= 1\n",
    "                n_wz[wj][z] -= 1\n",
    "                \n",
    "                z_prop = fromiter((z_posterior(n_z[z], n_wz[wi][z], \n",
    "                                               n_wz[wj][z]) for z in range(K)), \n",
    "                                  float, K)\n",
    "                z_probs = self._normalize(z_prop)\n",
    "                z_new = choice(K, p=z_probs)\n",
    "                \n",
    "                n_z[z_new] += 1\n",
    "                n_wz[wi][z_new] += 1\n",
    "                n_wz[wj][z_new] += 1\n",
    "                \n",
    "                current_assignments[i] = (bi, z_new)\n",
    "        \n",
    "        topic_words = {z: {word: phi_kw(z, word) for word in V} for z in range(K)}\n",
    "        topic_dist = fromiter((theta_z(z) for z in range(K)), float, K)\n",
    "        \n",
    "        return topic_dist, topic_words\n",
    "        \n",
    "    def get_topics(self):\n",
    "        \"\"\" \n",
    "        the global topic distribution \n",
    "        returns a list of K topic probabilities\n",
    "        \"\"\"\n",
    "        return self.topics\n",
    "        \n",
    "    def get_topic_words(self, n=5):\n",
    "        \"\"\"\n",
    "        the word distributions per topic\n",
    "        returns an ordered dict with \n",
    "        the top n most probable words of the topic\n",
    "        \"\"\"\n",
    "        sort_words = []\n",
    "        for z in range(self.ntopics):\n",
    "            word_prob = sorted(self.topic_words[z].items(), \n",
    "                               key=itemgetter(1), reverse=True)\n",
    "            sort_words.append(word_prob)\n",
    "            \n",
    "        topn = OrderedDict((z, dict(t[:n])) for z, t in enumerate(sort_words))\n",
    "        return topn\n",
    "    \n",
    "    def infer_documents(self):\n",
    "        \n",
    "        def p_zkbi(z, wi, wj):\n",
    "            p_z = self.topics[z]\n",
    "            p_wiz = self.topic_words[z][wi]\n",
    "            p_wjz = self.topic_words[z][wj]\n",
    "            return p_z * p_wiz *p_wjz\n",
    "        \n",
    "        doc_probs = {}\n",
    "        for d, doc in enumerate(self.text):\n",
    "            doc_biterms, _, _ = self._fit_corpus([doc])\n",
    "            \n",
    "            bcounts = Counter(doc_biterms)\n",
    "            nb = fromiter((bcounts[b] for b in doc_biterms), \n",
    "                          float, len(doc_biterms))\n",
    "            p_bd = self._normalize(nb)\n",
    "            \n",
    "            p_zb = zeros((self.ntopics, len(doc_biterms)))\n",
    "            for i, (wi, wj) in enumerate(doc_biterms):\n",
    "                zb = fromiter((p_zkbi(z, wi, wj) for z in range(self.ntopics)), \n",
    "                              float, self.ntopics)\n",
    "                p_zbi = self._normalize(zb)\n",
    "                p_zb[:, i] = p_zbi\n",
    "            \n",
    "            p_zd = p_zb.dot(p_bd)\n",
    "            doc_probs[d] = p_zd\n",
    "            \n",
    "        return doc_probs\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    with open('sampletext.txt', encoding='utf-8') as f:\n",
    "        text = f.read().lower().splitlines()\n",
    "    with open('stopwords.txt') as s:\n",
    "        stop = frozenset(s.read().splitlines())\n",
    "    \n",
    "    def clean(txt):\n",
    "        doc = simple_preprocess(txt)\n",
    "        return [d for d in doc if d not in stop]\n",
    "    \n",
    "    preprocessed = [clean(doc) for doc in text]\n",
    "    topic = BitermModel(preprocessed, ntopics=10, niter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a99742",
   "metadata": {},
   "source": [
    "### Topic Modeling with Minimal Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17013558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#词汇表\n",
    "words = list(np.asarray(text_pred.get_feature_names()))\n",
    "\n",
    "#加入锚定词汇，分别是汽车油耗、外观、噪音和空间这四个先验主题关键词列表\n",
    "anchor_words = [['油耗','省油'],\n",
    "['外观','外形','颜值','线条','前脸','时尚','造型','流畅'],\n",
    "['噪音','胎噪','噪音控制','隔音'],\n",
    "['空间','座位','拥挤']]\n",
    "\n",
    "\n",
    "# 训练带入先验知识的主题模型\n",
    "topic_model = tp.Coret(\n",
    "                        n_hidden=20 ,\n",
    "                        max_iter=100000,\n",
    "                        verbose=0,\n",
    "                        count='fraction',\n",
    "                        seed=2019\n",
    "                      )\n",
    "\n",
    "topic_model.fit(X_pro , #输入为稀疏词汇表示\n",
    "               words=words,\n",
    "               anchors = anchor_words,\n",
    "               anchor_strength=10  #锚定强度，数值越大，主题模型训练的结果受锚定词汇的影响就越大\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a03bef",
   "metadata": {},
   "source": [
    "### Dynamic Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652af3fc",
   "metadata": {},
   "source": [
    "https://github.com/derekgreene/dynamic-nmf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91d5bd",
   "metadata": {},
   "source": [
    "### Embedded Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ef85c",
   "metadata": {},
   "source": [
    "https://github.com/adjidieng/ETM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007df092",
   "metadata": {},
   "source": [
    "### LDA2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011f9be",
   "metadata": {},
   "source": [
    "https://github.com/meereeum/lda2vec-tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
