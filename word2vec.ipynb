{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0a2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块并设置日志记录\n",
    "import gensim, logging\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "import jieba\n",
    "\n",
    "# jieba.load_userdict(r\"...txt\")  #载入自定义词典，提高分词的准确性\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b935cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "'2018年10月份，麻省理工学院的Zakaria el Hjouji, D. Scott Hunter等学者发表了《The Impact of Bots on Opinions in Social Networks》，',\n",
    "'该研究通过分析 Twitter 上的机器人在舆论事件中的表现，证实了社交网络机器人可以对社交网络舆论产生很大的影响，不到消费者总数1%的活跃机器人，就可能左右整个舆论风向。',\n",
    "'麻省理工学院研究组的这项工作，最大的发现是，影响社交网络舆论所需要的机器人，其实是很少的。少数活跃的机器人，可以对网络舆论产生重大影响。',\n",
    "'机器人检测算法，会判断某用户是机器人的概率，但实际操作中研究者发现，该算法把几个经常转发但不常被@ 的真实用户也当做了机器人。所以研究者对有实名认证的 Twitter 用户做了筛查，把他们都归为真实用户。',\n",
    "'不论是真实的推特用户还是推特机器人，它们的三个基本操作是，关注、转发和评论（类似微博）。通过跟踪这些互动，研究者可以有效量化 Twitter 账号的行为。',\n",
    "'直觉上，那些不太关注别人的用户，也不太可能关注你。而且社交圈子重叠很重要，如果 A 和 B 是好友，那么关注 A 的用户就有较大概率关注 B。',\n",
    "'虽然人们在收到新信息时会更新他们的观点，但这个过程会随着时间的推移而减弱，他们的观点会逐渐变得顽固。Zaman 认为，如果你已经掌握了很多信息，你就越来越难听从别人的观点，新说法不会改变你的看法。',\n",
    "'该研究团队基于过往研究，给出了网络舆论模型的核心假设：',\n",
    "'社交网络中的个人是基于其朋友推文中的观点，来更新自身的观点；',\n",
    "'网络中某些用户的观点是顽固的，其观点不会轻易改变，而且顽固用户会推动其他用户（摇摆不定的中间派）改变观点。',\n",
    "'虽然社交媒体机器人不会带来物理威胁，但它们却可能有力影响到网络舆论。在微博里，各类水军已经经常出现在营销造势、危机公关中。虽然你能一眼识别出谁是水军，但仍然可能不知不觉地被他们影响。',\n",
    "'这些机器人看似僵尸，发起声来，比人类响亮得多，可能只要几十个几百个就足够扭转舆论！',\n",
    "'所以，从社会化媒体数据挖掘的角度来看，信息的真实性并不重要，只要文章、帖子或者评论能影响到浏览者或受众，具有一定的（潜在）影响力，这类社媒数据数据就值得去挖掘。',\n",
    "'更进一步说，跟销售数据反映消费者决策价值、搜索数据反映消费者意图价值相比，虽然社会化媒体文本数据的价值密度最低，好比是蕴藏金子和硅、却提炼极为困难的沙子，但由于它在互联网领域的分布极为广泛，',\n",
    "'且蕴含着对客观世界的细节描述和主观世界的宣泄（情绪、动机、心理等），其最大价值在于潜移默化地操控人的思想和行为的影响力，',\n",
    "'通过社会化媒体挖掘，我们可以得到对目标受众具有（潜在）影响力的商业情报。淘沙得金，排沙简金，最终得到的分析结果用以预判受众的思考和行为，为我们的生产实践服务。'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be958fc",
   "metadata": {},
   "source": [
    "英文规范化处理  \n",
    "-小写化  \n",
    "-词干提取  \n",
    "-词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "877593be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer() #词干分析\n",
    "wordnet_lemmatizer = WordNetLemmatizer() #词形还原\n",
    "sentences = [wordnet_lemmatizer.lemmatize(porter_stemmer.stem(i.lower())) for i  in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c05ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [jieba.lcut(x) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93f30ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:39:10,750 : INFO : collecting all words and their counts\n",
      "2022-03-25 16:39:10,752 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-03-25 16:39:10,754 : INFO : collected 314 word types from a corpus of 662 raw words and 16 sentences\n",
      "2022-03-25 16:39:10,755 : INFO : Loading a fresh vocabulary\n",
      "2022-03-25 16:39:10,759 : INFO : effective_min_count=0 retains 314 unique words (100% of original 314, drops 0)\n",
      "2022-03-25 16:39:10,761 : INFO : effective_min_count=0 leaves 662 word corpus (100% of original 662, drops 0)\n",
      "2022-03-25 16:39:10,765 : INFO : deleting the raw counts dictionary of 314 items\n",
      "2022-03-25 16:39:10,766 : INFO : sample=0.001 downsamples 92 most-common words\n",
      "2022-03-25 16:39:10,768 : INFO : downsampling leaves estimated 429 word corpus (64.9% of prior 662)\n",
      "2022-03-25 16:39:10,770 : INFO : estimated required memory for 314 words and 100 dimensions: 408200 bytes\n",
      "2022-03-25 16:39:10,771 : INFO : resetting layer weights\n",
      "2022-03-25 16:39:10,908 : INFO : training model with 3 workers on 314 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2022-03-25 16:39:10,914 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:10,917 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:10,919 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:10,920 : INFO : EPOCH - 1 : training on 662 raw words (427 effective words) took 0.0s, 51166 effective words/s\n",
      "2022-03-25 16:39:10,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:10,929 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:10,930 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:10,931 : INFO : EPOCH - 2 : training on 662 raw words (428 effective words) took 0.0s, 82538 effective words/s\n",
      "2022-03-25 16:39:10,937 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:10,939 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:10,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:10,941 : INFO : EPOCH - 3 : training on 662 raw words (438 effective words) took 0.0s, 113630 effective words/s\n",
      "2022-03-25 16:39:10,947 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:10,949 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:10,950 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:10,952 : INFO : EPOCH - 4 : training on 662 raw words (427 effective words) took 0.0s, 85883 effective words/s\n",
      "2022-03-25 16:39:10,958 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:10,959 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:10,961 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:10,962 : INFO : EPOCH - 5 : training on 662 raw words (423 effective words) took 0.0s, 94493 effective words/s\n",
      "2022-03-25 16:39:10,962 : INFO : training on a 3310 raw words (2143 effective words) took 0.1s, 40443 effective words/s\n",
      "2022-03-25 16:39:10,964 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# 在这些语句上训练word2vec模型\n",
    "model = gensim.models.Word2Vec(sentences,min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3338f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018', '年', '10', '月份', '，', '麻省理工学院', '的', 'zakaria', ' ', 'el', 'hjouji', ',', 'd', '.', 'scott', 'hunter', '等', '学者', '发表', '了', '《', 'the', 'impact', 'of', 'bots', 'on', 'opinions', 'in', 'social', 'networks', '》', '该', '研究', '通过', '分析', 'twitter', '上', '机器人', '在', '舆论', '事件', '中', '表现', '证实', '社交', '网络', '可以', '对', '产生', '很大', '影响', '不到', '消费者', '总数', '1%', '活跃', '就', '可能', '左右', '整个', '风向', '。', '研究组', '这项', '工作', '最大', '发现', '是', '所', '需要', '其实', '很少', '少数', '重大', '检测', '算法', '会', '判断', '某', '用户', '概率', '但', '实际操作', '研究者', '把', '几个', '经常', '转发', '不常', '被', '@', '真实', '也', '当做', '所以', '有', '实名', '认证', '做', '筛查', '他们', '都', '归', '为', '不论是', '推特', '还是', '它们', '三个', '基本操作', '关注', '、', '和', '评论', '（', '类似', '微博', '）', '跟踪', '这些', '互动', '有效', '量化', '账号', '行为', '直觉', '那些', '不太', '别人', '不太可能', '你', '而且', '社交圈子', '重叠', '很', '重要', '如果', 'a', 'b', '好友', '那么', '较大', '虽然', '人们', '收到', '新', '信息', '时会', '更新', '观点', '这个', '过程', '随着', '时间', '推移', '而', '减弱', '逐渐', '变得', '顽固', 'zaman', '认为', '已经', '掌握', '很多', '越来越', '难听', '从', '说法', '不会', '改变', '看法', '团队', '基于', '过往', '给出', '模型', '核心', '假设', '：', '个人', '其', '朋友', '推', '文中', '来', '自身', '；', '某些', '轻易', '推动', '其他', '摇摆不定', '中间派', '媒体', '带来', '物理', '威胁', '却', '有力', '到', '微', '博里', '各类', '水军', '经常出现', '营销', '造势', '危机', '公关', '能', '一眼', '识别', '出', '谁', '仍然', '不知不觉', '地被', '看似', '僵尸', '发起', '声来', '比', '人类', '响亮', '得', '多', '只要', '几十个', '几百个', '足够', '扭转', '！', '社会化', '数据挖掘', '角度', '来看', '真实性', '并', '不', '文章', '帖子', '或者', '浏览者', '或', '受众', '具有', '一定', '潜在', '影响力', '这类', '社媒', '数据', '值得', '去', '挖掘', '更进一步', '说', '跟', '销售', '反映', '决策', '价值', '搜索', '意图', '相比', '文本', '密度', '最低', '好比', '蕴藏', '金子', '硅', '提炼', '极为', '困难', '沙子', '由于', '它', '互联网', '领域', '分布', '广泛', '且', '蕴含着', '客观', '世界', '细节', '描述', '主观', '宣泄', '情绪', '动机', '心理', '在于', '潜移默化', '地', '操控', '人', '思想', '我们', '得到', '目标', '商业', '情报', '淘沙得金', '排沙简金', '最终', '结果', '用以', '预判', '思考', '生产实践', '服务']\n"
     ]
    }
   ],
   "source": [
    "print(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b218840b",
   "metadata": {},
   "source": [
    "另一种训练方式。先初始化词表，再进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d636073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:39:26,460 : INFO : collecting all words and their counts\n",
      "2022-03-25 16:39:26,462 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-03-25 16:39:26,464 : INFO : collected 314 word types from a corpus of 662 raw words and 16 sentences\n",
      "2022-03-25 16:39:26,466 : INFO : Loading a fresh vocabulary\n",
      "2022-03-25 16:39:26,468 : INFO : effective_min_count=0 retains 314 unique words (100% of original 314, drops 0)\n",
      "2022-03-25 16:39:26,470 : INFO : effective_min_count=0 leaves 662 word corpus (100% of original 662, drops 0)\n",
      "2022-03-25 16:39:26,474 : INFO : deleting the raw counts dictionary of 314 items\n",
      "2022-03-25 16:39:26,475 : INFO : sample=0.001 downsamples 92 most-common words\n",
      "2022-03-25 16:39:26,478 : INFO : downsampling leaves estimated 429 word corpus (64.9% of prior 662)\n",
      "2022-03-25 16:39:26,480 : INFO : estimated required memory for 314 words and 100 dimensions: 408200 bytes\n",
      "2022-03-25 16:39:26,481 : INFO : resetting layer weights\n",
      "2022-03-25 16:39:26,625 : INFO : training model with 3 workers on 314 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2022-03-25 16:39:26,630 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:26,633 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:26,634 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:26,635 : INFO : EPOCH - 1 : training on 662 raw words (427 effective words) took 0.0s, 83408 effective words/s\n",
      "2022-03-25 16:39:26,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:26,643 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:26,644 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:26,645 : INFO : EPOCH - 2 : training on 662 raw words (428 effective words) took 0.0s, 83853 effective words/s\n",
      "2022-03-25 16:39:26,653 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:26,655 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:26,657 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:26,659 : INFO : EPOCH - 3 : training on 662 raw words (438 effective words) took 0.0s, 59665 effective words/s\n",
      "2022-03-25 16:39:26,666 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:26,669 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:26,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:26,672 : INFO : EPOCH - 4 : training on 662 raw words (427 effective words) took 0.0s, 58705 effective words/s\n",
      "2022-03-25 16:39:26,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:26,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:26,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:26,684 : INFO : EPOCH - 5 : training on 662 raw words (423 effective words) took 0.0s, 73990 effective words/s\n",
      "2022-03-25 16:39:26,686 : INFO : training on a 3310 raw words (2143 effective words) took 0.1s, 35844 effective words/s\n",
      "2022-03-25 16:39:26,687 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2143, 3310)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = gensim.models.Word2Vec(min_count=0)  # 一个“空”的模型，还未进行实质性的训练\n",
    "new_model.build_vocab(sentences)                 # 可以是不可重复的，遍历一次语句生成器\n",
    "new_model.train(sentences, total_examples=new_model.corpus_count, epochs=5)   #可以是不可重复的，遍历一次语句生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c0f71",
   "metadata": {},
   "source": [
    "#### 模型训练中的重要参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5f908",
   "metadata": {},
   "source": [
    "min_count用于修剪内部字典。 试想，在十亿文字量的语料库中，只出现一次或两次的词汇很有可能是无趣的错别字和噪音信息。  \n",
    "此外，也没有足够的（上下文文本）数据对这些词汇进行任何有意义的训练，此时最好的策略就是忽略它们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c96c759d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:39:27,871 : INFO : collecting all words and their counts\n",
      "2022-03-25 16:39:27,873 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-03-25 16:39:27,875 : INFO : collected 314 word types from a corpus of 662 raw words and 16 sentences\n",
      "2022-03-25 16:39:27,876 : INFO : Loading a fresh vocabulary\n",
      "2022-03-25 16:39:27,880 : INFO : effective_min_count=0 retains 314 unique words (100% of original 314, drops 0)\n",
      "2022-03-25 16:39:27,882 : INFO : effective_min_count=0 leaves 662 word corpus (100% of original 662, drops 0)\n",
      "2022-03-25 16:39:27,887 : INFO : deleting the raw counts dictionary of 314 items\n",
      "2022-03-25 16:39:27,888 : INFO : sample=0.001 downsamples 92 most-common words\n",
      "2022-03-25 16:39:27,889 : INFO : downsampling leaves estimated 429 word corpus (64.9% of prior 662)\n",
      "2022-03-25 16:39:27,891 : INFO : estimated required memory for 314 words and 100 dimensions: 408200 bytes\n",
      "2022-03-25 16:39:27,892 : INFO : resetting layer weights\n",
      "2022-03-25 16:39:28,018 : INFO : training model with 3 workers on 314 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2022-03-25 16:39:28,025 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,028 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,029 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,031 : INFO : EPOCH - 1 : training on 662 raw words (427 effective words) took 0.0s, 56340 effective words/s\n",
      "2022-03-25 16:39:28,037 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,040 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,042 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,045 : INFO : EPOCH - 2 : training on 662 raw words (428 effective words) took 0.0s, 44588 effective words/s\n",
      "2022-03-25 16:39:28,050 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,053 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,056 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,058 : INFO : EPOCH - 3 : training on 662 raw words (438 effective words) took 0.0s, 52891 effective words/s\n",
      "2022-03-25 16:39:28,067 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,070 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,072 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,074 : INFO : EPOCH - 4 : training on 662 raw words (427 effective words) took 0.0s, 60216 effective words/s\n",
      "2022-03-25 16:39:28,081 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,083 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,084 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,086 : INFO : EPOCH - 5 : training on 662 raw words (423 effective words) took 0.0s, 65641 effective words/s\n",
      "2022-03-25 16:39:28,087 : INFO : training on a 3310 raw words (2143 effective words) took 0.1s, 31897 effective words/s\n",
      "2022-03-25 16:39:28,089 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "#  默认的min_count是5\n",
    "model =  gensim.models.Word2Vec(sentences, min_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfb570",
   "metadata": {},
   "source": [
    "size是gensim Word2Vec将词汇映射到的N维空间的维度数量（N）。  \n",
    "较大的size值需要更多的训练数据，但可以产生更好（更准确）的模型。合理的size数值介于在几十到几百之间。   \n",
    "如果你拥有的数据较少，那就把维度值设置小一点，这将在一定程度上减少模型的过拟合，尽量提高模型的表现效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47b9b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:39:28,675 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2022-03-25 16:39:28,678 : INFO : collecting all words and their counts\n",
      "2022-03-25 16:39:28,679 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-03-25 16:39:28,681 : INFO : collected 314 word types from a corpus of 662 raw words and 16 sentences\n",
      "2022-03-25 16:39:28,683 : INFO : Loading a fresh vocabulary\n",
      "2022-03-25 16:39:28,685 : INFO : effective_min_count=0 retains 314 unique words (100% of original 314, drops 0)\n",
      "2022-03-25 16:39:28,686 : INFO : effective_min_count=0 leaves 662 word corpus (100% of original 662, drops 0)\n",
      "2022-03-25 16:39:28,689 : INFO : deleting the raw counts dictionary of 314 items\n",
      "2022-03-25 16:39:28,689 : INFO : sample=0.001 downsamples 92 most-common words\n",
      "2022-03-25 16:39:28,692 : INFO : downsampling leaves estimated 429 word corpus (64.9% of prior 662)\n",
      "2022-03-25 16:39:28,692 : INFO : estimated required memory for 314 words and 50 dimensions: 282600 bytes\n",
      "2022-03-25 16:39:28,695 : INFO : resetting layer weights\n",
      "2022-03-25 16:39:28,841 : INFO : training model with 3 workers on 314 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2022-03-25 16:39:28,847 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,848 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,849 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,851 : INFO : EPOCH - 1 : training on 662 raw words (427 effective words) took 0.0s, 84580 effective words/s\n",
      "2022-03-25 16:39:28,857 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,859 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,860 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,861 : INFO : EPOCH - 2 : training on 662 raw words (428 effective words) took 0.0s, 126138 effective words/s\n",
      "2022-03-25 16:39:28,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,871 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,873 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,874 : INFO : EPOCH - 3 : training on 662 raw words (438 effective words) took 0.0s, 59924 effective words/s\n",
      "2022-03-25 16:39:28,883 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,886 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,887 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,888 : INFO : EPOCH - 4 : training on 662 raw words (427 effective words) took 0.0s, 88294 effective words/s\n",
      "2022-03-25 16:39:28,895 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:39:28,898 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:39:28,899 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:39:28,901 : INFO : EPOCH - 5 : training on 662 raw words (423 effective words) took 0.0s, 60709 effective words/s\n",
      "2022-03-25 16:39:28,902 : INFO : training on a 3310 raw words (2143 effective words) took 0.1s, 35871 effective words/s\n",
      "2022-03-25 16:39:28,904 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "#  默认的size数是100\n",
    "model = gensim.models.Word2Vec(sentences, size=50, min_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ac3c8",
   "metadata": {},
   "source": [
    "workers是一个用于训练并行化的参数，可以加快训练速度："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b6299",
   "metadata": {},
   "source": [
    "iter是模型训练时在整个训练语料库上的迭代次数，假如参与训练的文本量较少，就需要把这个参数调大一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ef7a9",
   "metadata": {},
   "source": [
    "sg是模型训练所采用的的算法类型：  \n",
    "1 代表 skip-gram，该模型从上下文语境（context）对目标词汇（target word）的预测中学习到其词向量的表达  \n",
    "0代表 CBOW，该模型从目标词汇（target word）对上下文语境（context）的预测中学习到其词向量的表达"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49517d5d",
   "metadata": {},
   "source": [
    "window控制窗口，它指当前词和预测词之间的最大距离，如果设得较小，那么模型学习到的是词汇间的组合性关系（词性相异）  \n",
    "假如语料够多，笔者一般会设置得大一些，8~10，因为词汇间的聚合关系能很好的捕捉到词汇之间的相似性关系，能很好的解决词袋表示中多词一义的问题，发现词汇/语句之间的潜在语义相关性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84862b3e",
   "metadata": {},
   "source": [
    "#### 模型序列化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f611a91",
   "metadata": {},
   "source": [
    "可以使用标准gensim方法存储和加载模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(save_path)  # 保存模型\n",
    "\n",
    "new_model = gensim.models.Word2Vec.load(save_path)  # 加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a94fe6",
   "metadata": {},
   "source": [
    "#### 在线训练/增量学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8436898",
   "metadata": {},
   "source": [
    "当有了新的数据，就可以直接在原来已经训练好的模型上接着训练，而不用从头再来，后续可以不断加入新的语句（经过文本预处理）来提升模型的表现效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb87ab69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1ef99e7d2e8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model #目前有一个已经训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad955319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 50)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "597668a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:40:13,676 : INFO : collecting all words and their counts\n",
      "2022-03-25 16:40:13,679 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-03-25 16:40:13,680 : INFO : collected 111 word types from a corpus of 195 raw words and 8 sentences\n",
      "2022-03-25 16:40:13,683 : INFO : Updating model with new vocabulary\n",
      "2022-03-25 16:40:13,685 : INFO : New added 111 unique words (50% of original 222) and increased the count of 111 pre-existing words (50% of original 222)\n",
      "2022-03-25 16:40:13,689 : INFO : deleting the raw counts dictionary of 111 items\n",
      "2022-03-25 16:40:13,691 : INFO : sample=0.001 downsamples 222 most-common words\n",
      "2022-03-25 16:40:13,692 : INFO : downsampling leaves estimated 164 word corpus (84.5% of prior 195)\n",
      "2022-03-25 16:40:13,695 : INFO : estimated required memory for 222 words and 50 dimensions: 199800 bytes\n",
      "2022-03-25 16:40:13,696 : INFO : updating layer weights\n",
      "2022-03-25 16:40:13,743 : INFO : training model with 3 workers on 388 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2022-03-25 16:40:13,752 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:40:13,755 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:40:13,757 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:40:13,759 : INFO : EPOCH - 1 : training on 195 raw words (95 effective words) took 0.0s, 9257 effective words/s\n",
      "2022-03-25 16:40:13,767 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:40:13,770 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:40:13,773 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:40:13,778 : INFO : EPOCH - 2 : training on 195 raw words (80 effective words) took 0.0s, 6381 effective words/s\n",
      "2022-03-25 16:40:13,785 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:40:13,788 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:40:13,789 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:40:13,791 : INFO : EPOCH - 3 : training on 195 raw words (78 effective words) took 0.0s, 10295 effective words/s\n",
      "2022-03-25 16:40:13,798 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:40:13,800 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:40:13,802 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:40:13,803 : INFO : EPOCH - 4 : training on 195 raw words (83 effective words) took 0.0s, 11353 effective words/s\n",
      "2022-03-25 16:40:13,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:40:13,817 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:40:13,818 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:40:13,820 : INFO : EPOCH - 5 : training on 195 raw words (89 effective words) took 0.0s, 11089 effective words/s\n",
      "2022-03-25 16:40:13,821 : INFO : training on a 975 raw words (425 effective words) took 0.1s, 5524 effective words/s\n",
      "2022-03-25 16:40:13,822 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(425, 975)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_sentences = [\n",
    "    '众所周知，社交聆听是一个需要社交媒体聆听/社交媒体监控工具的实现的过程。',\n",
    "    '打开应用程序时，您要做的第一件事就是输入要监控的关键字。','关键字是最能描述您想要在社交媒体平台和网络上找到的内容的词。',\n",
    "    '关键字可以是一个单词（例如“飞利浦”），两个单词，四个单词（例如“搜索引擎优化工具”）等。',\n",
    "    '这些示例中的每一个都呈现一个关键字。输入关键字后，该工具会搜索这些关键字的提及次数并将其收集在一个位置。',\n",
    "    '监控营销（和其他）活动以及产品发布至关重要。',\n",
    "    '社交媒体上的反应非常迅速。只有通过实时监控此类事件，您才会立即知道它是否正常运行以及是否存在您在创建活动时可能没有注意到的问题。',\n",
    "    '你越早知道越好。要监控广告系列，请输入其名称（如果有），标语和/或主题标签作为关键字。'  \n",
    "        ]\n",
    "\n",
    "more_sentences = [jieba.lcut(i) for i  in more_sentences]\n",
    "\n",
    "#注意该方法中的参数update，默认为False,增量更新模型时，需要设置为True\n",
    "model.build_vocab(more_sentences, update=True)  \n",
    "\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f38f92fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae640e0e",
   "metadata": {},
   "source": [
    "Word2vec模型的常用方法枚举"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71fd8ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 50)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词向量的维度\n",
    "model.wv.vectors.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3607b9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018', '年', '10', '月份', '，', '麻省理工学院', '的', 'zakaria', ' ', 'el', 'hjouji', ',', 'd', '.', 'scott', 'hunter', '等', '学者', '发表', '了', '《', 'the', 'impact', 'of', 'bots', 'on', 'opinions', 'in', 'social', 'networks', '》', '该', '研究', '通过', '分析', 'twitter', '上', '机器人', '在', '舆论', '事件', '中', '表现', '证实', '社交', '网络', '可以', '对', '产生', '很大', '影响', '不到', '消费者', '总数', '1%', '活跃', '就', '可能', '左右', '整个', '风向', '。', '研究组', '这项', '工作', '最大', '发现', '是', '所', '需要', '其实', '很少', '少数', '重大', '检测', '算法', '会', '判断', '某', '用户', '概率', '但', '实际操作', '研究者', '把', '几个', '经常', '转发', '不常', '被', '@', '真实', '也', '当做', '所以', '有', '实名', '认证', '做', '筛查', '他们', '都', '归', '为', '不论是', '推特', '还是', '它们', '三个', '基本操作', '关注', '、', '和', '评论', '（', '类似', '微博', '）', '跟踪', '这些', '互动', '有效', '量化', '账号', '行为', '直觉', '那些', '不太', '别人', '不太可能', '你', '而且', '社交圈子', '重叠', '很', '重要', '如果', 'a', 'b', '好友', '那么', '较大', '虽然', '人们', '收到', '新', '信息', '时会', '更新', '观点', '这个', '过程', '随着', '时间', '推移', '而', '减弱', '逐渐', '变得', '顽固', 'zaman', '认为', '已经', '掌握', '很多', '越来越', '难听', '从', '说法', '不会', '改变', '看法', '团队', '基于', '过往', '给出', '模型', '核心', '假设', '：', '个人', '其', '朋友', '推', '文中', '来', '自身', '；', '某些', '轻易', '推动', '其他', '摇摆不定', '中间派', '媒体', '带来', '物理', '威胁', '却', '有力', '到', '微', '博里', '各类', '水军', '经常出现', '营销', '造势', '危机', '公关', '能', '一眼', '识别', '出', '谁', '仍然', '不知不觉', '地被', '看似', '僵尸', '发起', '声来', '比', '人类', '响亮', '得', '多', '只要', '几十个', '几百个', '足够', '扭转', '！', '社会化', '数据挖掘', '角度', '来看', '真实性', '并', '不', '文章', '帖子', '或者', '浏览者', '或', '受众', '具有', '一定', '潜在', '影响力', '这类', '社媒', '数据', '值得', '去', '挖掘', '更进一步', '说', '跟', '销售', '反映', '决策', '价值', '搜索', '意图', '相比', '文本', '密度', '最低', '好比', '蕴藏', '金子', '硅', '提炼', '极为', '困难', '沙子', '由于', '它', '互联网', '领域', '分布', '广泛', '且', '蕴含着', '客观', '世界', '细节', '描述', '主观', '宣泄', '情绪', '动机', '心理', '在于', '潜移默化', '地', '操控', '人', '思想', '我们', '得到', '目标', '商业', '情报', '淘沙得金', '排沙简金', '最终', '结果', '用以', '预判', '思考', '生产实践', '服务', '众所周知', '聆听', '一个', '/', '监控', '工具', '实现', '打开', '应用程序', '时', '您', '要', '第一件', '事', '就是', '输入', '关键字', '最能', '想要', '平台', '找到', '内容', '词', '单词', '例如', '“', '飞利浦', '”', '两个', '四个', '搜索引擎', '优化', '示例', '每', '呈现', '后', '提及', '次数', '将', '收集', '位置', '活动', '以及', '产品', '发布', '至关重要', '反应', '非常', '迅速', '只有', '实时', '此类', '才', '立即', '知道', '是否', '正常', '运行', '存在', '创建活动', '没有', '注意', '问题', '越', '早', '好', '广告', '系列', '请', '名称', '标语', '主题', '标签', '作为']\n"
     ]
    }
   ],
   "source": [
    "# 词汇列表\n",
    "print(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87174bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:40:45,748 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('证实', 0.4277941584587097),\n",
       " ('研究组', 0.35853835940361023),\n",
       " ('发表', 0.3377939760684967),\n",
       " ('人类', 0.3369925022125244),\n",
       " ('会', 0.3299635648727417),\n",
       " ('轻易', 0.326449453830719),\n",
       " ('在于', 0.3105165958404541),\n",
       " ('认证', 0.3091813325881958),\n",
       " ('广泛', 0.3022865056991577),\n",
       " ('研究者', 0.29552140831947327),\n",
       " ('筛查', 0.2948150634765625),\n",
       " ('潜移默化', 0.29432496428489685),\n",
       " ('通过', 0.2928626835346222),\n",
       " ('scott', 0.2926477789878845),\n",
       " ('搜索引擎', 0.2899874150753021),\n",
       " ('zakaria', 0.2857602536678314),\n",
       " ('信息', 0.2735777199268341),\n",
       " ('才', 0.27169063687324524),\n",
       " ('上', 0.2680913805961609),\n",
       " ('金子', 0.2639997899532318)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取词汇相关的前n个词语，当positive和negative同时使用的话，就是词汇类比 (Word Analogy )。\n",
    "model.wv.most_similar(positive=['网络'], negative=['产品'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f4ba0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:40:48,089 : WARNING : vectors for words {'场景', '咨询', '舆情'} are not present in the model, ignoring these words\n",
      "C:\\Users\\18438\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'互联网'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出与其他词差异最大的词汇\n",
    "model.wv.doesnt_match(\"舆情 互联网 媒体 商业 场景 咨询 \".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef490026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['行为', '重大', '；', '搜索', '非常']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接近词汇A更甚于词汇B接近词汇A的【所有】词汇,按相似度由高到低降序排列\n",
    "model.wv.closer_than('标签','主题')  #'微博'是词汇A,'社会化媒体'是词汇B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d87b5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用similar_by_word方法获得的相似词汇结果：\n",
      " [('，', 0.3393872380256653), ('观点', 0.2451881617307663), ('可以', 0.21895641088485718), ('影响', 0.16901680827140808), ('在', 0.1647396683692932), ('）', 0.1644335240125656), ('机器人', 0.14234238862991333), ('。', 0.1311880201101303), ('虽然', 0.12112195789813995), ('网络', 0.11125838756561279)]\n"
     ]
    }
   ],
   "source": [
    "# 找到前N个最相似的单词，注意其中的参数restrict_vocab ，它是可选的整数，它限制了向量的范围，搜索最相似的值。 \n",
    "# 例如，restrict_vocab = 10000会，只检查词汇顺序（按降序频率对词汇表进行排序会更有有意义）中的前10000个词汇向量。\n",
    "print('用similar_by_word方法获得的相似词汇结果：\\n',model.wv.similar_by_word('信息', topn=10, restrict_vocab=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1233f90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用most_similar方法获得的相似词汇结果：\n",
      " [('系列', 0.3889453709125519), ('最低', 0.3657423257827759), ('时间', 0.3597252368927002), ('事件', 0.354660302400589), ('把', 0.33942291140556335), ('，', 0.3393872380256653), ('顽固', 0.331727534532547), ('不', 0.32419413328170776), ('真实性', 0.2922455072402954), ('某', 0.27310267090797424)]\n"
     ]
    }
   ],
   "source": [
    "print('用most_similar方法获得的相似词汇结果：\\n',model.wv.most_similar('信息'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9924ddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'微博'和'数据'之间的词汇相似度为：-0.05364808440208435\n"
     ]
    }
   ],
   "source": [
    "# 基于cosine余弦计算词汇之间的相似度，数值越大代表相似度越高\n",
    "print(\"'微博'和'数据'之间的词汇相似度为：{}\".format(model.wv.similarity('微博', '数据')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4055df01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('能', 0.0025773784), ('极为', 0.0025773754), ('，', 0.0025773717), ('改变', 0.0025773658), ('所以', 0.0025773593), ('信息', 0.0025773584), ('是', 0.0025773575), ('商业', 0.0025773556), ('排沙简金', 0.0025773542), ('一眼', 0.0025773542), ('他们', 0.0025773537), ('。', 0.0025773533), ('不', 0.0025773523), ('帖子', 0.0025773514), ('对', 0.00257735), ('转发', 0.002577349), ('其', 0.0025773488), ('淘沙得金', 0.0025773481), ('twitter', 0.0025773467), ('心理', 0.0025773465)]\n"
     ]
    }
   ],
   "source": [
    "#  给定上下文词汇（the context words）作为输入，你可以获得中心词汇的概率分布\n",
    "print(model.predict_output_word(['口碑', '情报'], topn=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3de443b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00016201, -0.00807795,  0.00742391,  0.00403288,  0.00265464,\n",
       "       -0.00712455, -0.00398818,  0.00918999,  0.00884788, -0.00440946,\n",
       "        0.00081052,  0.00878195, -0.00017232, -0.00891069, -0.0007567 ,\n",
       "       -0.00273111,  0.00146872,  0.00574722, -0.00898216, -0.00862026,\n",
       "       -0.00470968,  0.00067314,  0.0027353 ,  0.00864497,  0.00228916,\n",
       "        0.00019195, -0.00210022,  0.00810323,  0.00907752, -0.0059774 ,\n",
       "       -0.00229304,  0.00734922,  0.00060637, -0.00964607, -0.00806757,\n",
       "        0.00533147, -0.00235929,  0.00647586,  0.00372188,  0.00346097,\n",
       "       -0.00974079, -0.00865499, -0.00809868,  0.00688452, -0.00298843,\n",
       "        0.00763888,  0.00933538, -0.00469522,  0.00172749,  0.00461897],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查询到某个词汇的词向量稠密表示\n",
    "model.wv['商业']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6e81a",
   "metadata": {},
   "source": [
    "训练损失计算(Training Loss Computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78674d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 16:45:20,863 : INFO : collecting all words and their counts\n",
      "2022-03-25 16:45:20,865 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-03-25 16:45:20,866 : INFO : collected 314 word types from a corpus of 662 raw words and 16 sentences\n",
      "2022-03-25 16:45:20,868 : INFO : Loading a fresh vocabulary\n",
      "2022-03-25 16:45:20,870 : INFO : effective_min_count=1 retains 314 unique words (100% of original 314, drops 0)\n",
      "2022-03-25 16:45:20,872 : INFO : effective_min_count=1 leaves 662 word corpus (100% of original 662, drops 0)\n",
      "2022-03-25 16:45:20,877 : INFO : deleting the raw counts dictionary of 314 items\n",
      "2022-03-25 16:45:20,879 : INFO : sample=0.001 downsamples 92 most-common words\n",
      "2022-03-25 16:45:20,880 : INFO : downsampling leaves estimated 429 word corpus (64.9% of prior 662)\n",
      "2022-03-25 16:45:20,883 : INFO : estimated required memory for 314 words and 100 dimensions: 408200 bytes\n",
      "2022-03-25 16:45:20,885 : INFO : resetting layer weights\n",
      "2022-03-25 16:45:21,044 : INFO : training model with 3 workers on 314 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2022-03-25 16:45:21,053 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:45:21,055 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:45:21,057 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:45:21,058 : INFO : EPOCH - 1 : training on 662 raw words (456 effective words) took 0.0s, 60285 effective words/s\n",
      "2022-03-25 16:45:21,065 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:45:21,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:45:21,070 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:45:21,072 : INFO : EPOCH - 2 : training on 662 raw words (420 effective words) took 0.0s, 51285 effective words/s\n",
      "2022-03-25 16:45:21,076 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:45:21,078 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:45:21,080 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:45:21,083 : INFO : EPOCH - 3 : training on 662 raw words (425 effective words) took 0.0s, 57299 effective words/s\n",
      "2022-03-25 16:45:21,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:45:21,089 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:45:21,090 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:45:21,092 : INFO : EPOCH - 4 : training on 662 raw words (424 effective words) took 0.0s, 79094 effective words/s\n",
      "2022-03-25 16:45:21,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-25 16:45:21,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-25 16:45:21,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-25 16:45:21,102 : INFO : EPOCH - 5 : training on 662 raw words (425 effective words) took 0.0s, 72595 effective words/s\n",
      "2022-03-25 16:45:21,103 : INFO : training on a 3310 raw words (2150 effective words) took 0.1s, 37810 effective words/s\n",
      "2022-03-25 16:45:21,104 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50069.953125\n"
     ]
    }
   ],
   "source": [
    "# 实例化并训练Word2Vec模型\n",
    "model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=2019) # 获得训练的损失值\n",
    "training_loss = model_with_loss.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e6d332",
   "metadata": {},
   "source": [
    "我们希望在生产中仍然能提高模型的训练性能， 一个好方法是在字典中缓存所有相似的词汇。   \n",
    "因此，当我们下次得到类似的查询词汇时，我们将首先在词典中搜索它。 如果命中，那么我们将直接从词典中显示结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "394d6b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与【，】最相关的词汇是：\n",
      "[('想要', 0.40473735332489014), ('顽固', 0.3838478624820709), ('正常', 0.3547285199165344), ('信息', 0.3393872380256653), ('研究组', 0.32838302850723267), ('人们', 0.32351717352867126), ('核心', 0.3221820592880249), ('得', 0.30716368556022644), ('风向', 0.30555838346481323), ('几个', 0.2919811010360718)]\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "与【的】最相关的词汇是：\n",
      "[('示例', 0.4537862241268158), ('请', 0.42197251319885254), ('机器人', 0.3583510220050812), ('掌握', 0.3498440086841583), ('蕴藏', 0.34529995918273926), ('用户', 0.31492969393730164), ('顽固', 0.3008692264556885), ('客观', 0.2846525311470032), ('。', 0.2815815210342407), ('bots', 0.2776123881340027)]\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "与【 】最相关的词汇是：\n",
      "[('b', 0.4699980914592743), ('作为', 0.3454782962799072), ('产生', 0.3051154315471649), ('随着', 0.30277836322784424), ('它们', 0.3023892939090729), ('帖子', 0.29358670115470886), ('找到', 0.2761099636554718), ('细节', 0.2726961374282837), ('推', 0.2693916857242584), ('潜在', 0.26805639266967773)]\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "与【。】最相关的词汇是：\n",
      "[('示例', 0.36426952481269836), ('等', 0.361717164516449), ('数据挖掘', 0.3600717782974243), ('量化', 0.35197123885154724), ('领域', 0.34416109323501587), ('动机', 0.34069395065307617), ('迅速', 0.33818235993385315), ('实名', 0.33165472745895386), ('较大', 0.3289051651954651), ('造势', 0.32013431191444397)]\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "与【机器人】最相关的词汇是：\n",
      "[('）', 0.37148258090019226), ('的', 0.3583510220050812), ('说', 0.3370071053504944), ('越来越', 0.3206462562084198), ('情绪', 0.3199811279773712), ('；', 0.31177160143852234), ('创建活动', 0.299083411693573), ('麻省理工学院', 0.29536446928977966), ('掌握', 0.2952904999256134), ('活动', 0.28797510266304016)]\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n"
     ]
    }
   ],
   "source": [
    "most_similars_precalc = {word : model.wv.most_similar(word) for word in model.wv.index2word}\n",
    "for i, (key, value) in enumerate(most_similars_precalc.items()):  \n",
    "    if i==5:    \n",
    "        break  \n",
    "    print('与【{}】最相关的词汇是：\\n{}'.format(key,value))  \n",
    "    print('————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a824f",
   "metadata": {},
   "source": [
    "#### word2vec模型用于后续任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50a7ff",
   "metadata": {},
   "source": [
    "方向1 词汇相似度度量  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4b210",
   "metadata": {},
   "source": [
    "使用get_keras_embedding方法返回可用的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed851972",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv\n",
    "embedding_layer = wv.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd146f2",
   "metadata": {},
   "source": [
    "接下来，我们构建Keras模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = Input(shape=(1,), dtype='int32', name='input_a')\n",
    "input_b = Input(shape=(1,), dtype='int32', name='input_b')\n",
    "embedding_a = embedding_layer(input_a)\n",
    "embedding_b = embedding_layer(input_b)\n",
    "similarity = dot([embedding_a, embedding_b], axes=2, normalize=True)\n",
    "keras_model = Model(inputs=[input_a, input_b], outputs=similarity)\n",
    "keras_model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb386d",
   "metadata": {},
   "source": [
    "现在，输入两个要进行比较的词汇，模型返回的数值就是这两个词汇的相似度，数值越大表示两个词汇的含义越接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = '文本挖掘'\n",
    "word_b = 'social_listening'# 输出是两个词之间的余弦距离（一种文本相似性度量）\n",
    "output = keras_model.predict([np.asarray([model.wv.vocab[word_a].index]), np.asarray([model.wv.vocab[word_b].index])])\n",
    "print (output[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e1669",
   "metadata": {},
   "source": [
    "方向2 文本分类  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879220a",
   "metadata": {},
   "source": [
    "下一步，准备好嵌入层，以便在实际的keras模型中运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keras_w2v = word2vec.Word2Vec(min_count=1,size = 50)\n",
    "Keras_w2v.build_vocab(texts_w2v)\n",
    "Keras_w2v.train(texts, total_examples=Keras_w2v.corpus_count, epochs=Keras_w2v.epochs)\n",
    "Keras_w2v_wv = Keras_w2v.wv\n",
    "embedding_layer = Keras_w2v_wv.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('构建模型...')\n",
    "model = Sequential()\n",
    "\n",
    "#我们从一个有效的嵌入层开始，它将训练文本中的词汇索引映射到嵌入维度中\n",
    "input_shape=(MAX_SEQUENCE_LENGTH,)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# 我们添加卷积1d\n",
    "model.add(Conv1D(256,5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "# 使用全局最大池化层（global max pooling）\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# 增加一个隐藏层\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#我们投射到一个单元输出层，使用适用于多分类的激活函数 - softmax：\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
